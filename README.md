# Pruning Optimization Based Federated Training Algorithm of Large Models

Federated learning is a new type of distributed machine learning paradigm, which can use the local data and computing resources of many edge devices to train large federated model on the basis of privacy protection. However, in the scenario of federated learning, the communication bandwidth and computing resources of edge devices are usually limited, so the federated large model needs to be trained and reasoned on edge devices after model compression. In addition, the data distribution between clients in a federated learning scenario is usually non-independent and identically distributed, and may contain label noise, resulting in differences in the quality of sample labels between clients, which affects the training effect of the federated large model. This paper proposes a novel large model federation training algorithm based on pruning optimization. This algorithm uses model pruning based on dynamic topology to compress the federated large model. After local training on the client, all pruned models are aggregated based on contribution evaluation, and finally the pruned model is restored to a federated large model based on model residuals. In the case of differences in client sample labels, compared with other baseline algorithms, the large model federated training algorithm based on pruning optimization proposed in this paper can achieve optimal performance while reducing the overhead of communication and computing resources.
